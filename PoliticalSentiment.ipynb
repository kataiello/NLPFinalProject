{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Need to calculate the following:\n",
    "Classifier:\n",
    "P(s|T) = (P(s)*P(T|s))/P(T)\n",
    "T is the text of the tweet and s is the sentiment\n",
    "\n",
    "P(T|s) = P(G|s) = product(for every g in G)(P(g|s))\n",
    "Where G is the set of n-grams and g is an individual n-gram\n",
    "Calculate P(g|s) using +1 for 0 values\n",
    "\n",
    "Calculate P(T) for all T\n",
    "Then add all g to sets s with counts\n",
    "'''\n",
    "\n",
    "\n",
    "#haven't collected corpus yet, assume this will be filled in\n",
    "#Load processed tweets here\n",
    "with open('hillProcessed.txt') as f:\n",
    "    hillProcessed = f.read()\n",
    "hillTweets = hillProcessed.split('\\n')\n",
    "\n",
    "with open('trumpProcessed.txt') as f:\n",
    "    trumpProcessed = f.read()\n",
    "trumpTweets = trumpProcessed.split('\\n')\n",
    "\n",
    "#Load tokens files for bigrams\n",
    "with open('hill_tokens.txt') as f:\n",
    "    hillTokensFile = f.read()\n",
    "    \n",
    "hillTokens = hillTokensFile.strip().split('\\n')\n",
    "\n",
    "with open('trump_tokens.txt') as f:\n",
    "    trumpTokensFile = f.read()\n",
    "    \n",
    "trumpTokens = trumpTokensFile.strip().split('\\n')\n",
    "\n",
    "#LOAD COUNTS OF TYPES IN EACH HERE AS HILLTYPES AND TRUMPTYPES\n",
    "with open('hill_counts.txt') as f:\n",
    "    hCountsFile = f.read()\n",
    "hCounts = hCountsFile.strip().split('\\n')\n",
    "hillTypes = {}\n",
    "for i in range (len(hCounts)):\n",
    "    hCounts[i] = hCounts[i].lstrip()\n",
    "    hillTypes[hCounts[i].split(' ')[1]] = int(hCounts[i].split(' ')[0]) \n",
    "    \n",
    "with open('hill_counts.txt') as f:\n",
    "    hCountsFile = f.read()\n",
    "hCounts = hCountsFile.strip().split('\\n')\n",
    "hillTypes = {}\n",
    "for i in range (len(hCounts)):\n",
    "    hCounts[i] = hCounts[i].lstrip()\n",
    "    hillTypes[hCounts[i].split(' ')[1]] = int(hCounts[i].split(' ')[0]) \n",
    "    \n",
    "with open('trump_counts.txt') as f:\n",
    "    tCountsFile = f.read()\n",
    "tCounts = tCountsFile.strip().split('\\n')\n",
    "trumpTypes = {}\n",
    "for i in range (len(tCounts)):\n",
    "    tCounts[i] = tCounts[i].lstrip()\n",
    "    trumpTypes[tCounts[i].split(' ')[1]] = int(tCounts[i].split(' ')[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def runAnalysis(testFile):\n",
    "    with open(testFile) as f:\n",
    "        for line in f:\n",
    "            pHill, pTrump = classify(line)\n",
    "            print (\"For tweet: \" + line + \" \\nProbability of supporting \\n Hillary Clinton: \" + pHill + \"\\n Donald Trump: \" + pTrump)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for every word, add it to the dictionary in this format:\n",
    "#key is the word, value is another dictionary\n",
    "    #within each subdictionary, have the following word as the key\n",
    "    #and the number of times it follows as the value\n",
    "\n",
    "def generateBigramsDict():\n",
    "    #make a bigrams dictionary for both and each candidate\n",
    "    bigrams = {}\n",
    "    hillBigrams = {}\n",
    "    trumpBigrams = {}\n",
    "    #Add all Hillary positive bigrams to both hillBigrams and bigrams\n",
    "    for i in range(len(hillTokens) - 1):\n",
    "        #if the word already exists\n",
    "        if hillTokens[i] in hillBigrams:\n",
    "            #if the following word already exists\n",
    "            if hillTokens[i+1] in hillBigrams[hillTokens[i]]:\n",
    "                #add 1 to the number of occurences\n",
    "                hillBigrams[hillTokens[i]][hillTokens[i+1]] += 1\n",
    "                bigrams[hillTokens[i]][hillTokens[i+1]] += 1\n",
    "            #add it to the subdictionary of hill\n",
    "            else:\n",
    "                hillBigrams[hillTokens[i]][hillTokens[i+1]] = 1\n",
    "                bigrams[hillTokens[i]][hillTokens[i+1]] = 1\n",
    "        #the word does not exist yet in hillBigrams\n",
    "        else:\n",
    "            #initialize a new subdictionary and add the first bigram\n",
    "            hillBigrams[hillTokens[i]] = {hillTokens[i+1]: 1}\n",
    "            bigrams[hillTokens[i]] = {hillTokens[i+1]: 1}\n",
    "\n",
    "    #Add all Trump positive bigrams to both trumpBigrams and bigrams\n",
    "    for i in range(len(trumpTokens) - 1):\n",
    "        #if the word already exists\n",
    "        if trumpTokens[i] in trumpBigrams:\n",
    "            #if the following word already exists\n",
    "            if trumpTokens[i+1] in trumpBigrams[trumpTokens[i]]:\n",
    "                #add 1 to the number of occurences\n",
    "                trumpBigrams[trumpTokens[i]][trumpTokens[i+1]] += 1\n",
    "                bigrams[trumpTokens[i]][trumpTokens[i+1]] += 1\n",
    "            #add it to the subdictionary of trump\n",
    "            else:\n",
    "                trumpBigrams[trumpTokens[i]][trumpTokens[i+1]] = 1\n",
    "                #check bigrams\n",
    "                if trumpTokens[i+1] in bigrams[trumpTokens[i]]:\n",
    "                    bigrams[trumpTokens[i]][trumpTokens[i+1]] += 1\n",
    "                else:\n",
    "                    bigrams[trumpTokens[i]][trumpTokens[i+1]] = 1\n",
    "        #the word does not exist yet in trumpBigrams\n",
    "        else:\n",
    "            #initialize a new subdictionary and add the first bigram\n",
    "            trumpBigrams[trumpTokens[i]] = {trumpTokens[i+1]: 1}\n",
    "            #check bigrams\n",
    "            if trumpTokens[i] in bigrams:\n",
    "                #check word\n",
    "                if trumpTokens[i+1] in bigrams[trumpTokens[i]]:\n",
    "                    bigrams[trumpTokens[i]][trumpTokens[i+1]] += 1\n",
    "                else:\n",
    "                    bigrams[trumpTokens[i]][trumpTokens[i+1]] = 1\n",
    "            else:\n",
    "                bigrams[trumpTokens[i]] = {trumpTokens[i+1]: 1}\n",
    "    return bigrams, hillBigrams, trumpBigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import float32\n",
    "from math import log, exp\n",
    "\n",
    "'''Need to implement Laplace smoothing'''\n",
    "\n",
    "\n",
    "'''MLE = count of bigram / count of first unigram\n",
    "Laplace: P = (count of bigram + 1)/(count of first unigram + V)\n",
    "where V is the number of possible bigrams given unigram\n",
    "What do we do if the given unigram doesn't exist?'''\n",
    "\n",
    "def smoothProb(word1, word2, sentiment):\n",
    "    P = numpy.float32(0.0)\n",
    "  \n",
    "    #True = hillary\n",
    "    if sentiment == True:\n",
    "        unigramCount = 1\n",
    "        #if the unigram has been seen before\n",
    "        if word1 in hBigrams.keys():\n",
    "            #add one for smoothing\n",
    "            unigramCount = hillTypes[word1] + 1\n",
    "            #if this bigram has been seen before\n",
    "            if word2 in hBigrams[word1].keys():\n",
    "                P = (hBigrams[word1][word2] + 1) / (unigramCount + len(hBigrams[word1]))\n",
    "                #maybe log it? FIGURE OUT LATER\n",
    "            #the second word has not followed the first word before\n",
    "            else:\n",
    "                P = 1 / (unigramCount + len(hBigrams[word1]))\n",
    "        #this unigram has not been seen before\n",
    "        else:\n",
    "            P = unigramCount / (len(hillTokens)+len(hillTypes))\n",
    "        return P\n",
    "    #False = trump\n",
    "    else:\n",
    "        unigramCount = 1\n",
    "        #if the unigram has been seen before\n",
    "        if word1 in tBigrams.keys():\n",
    "            #add one for smoothing\n",
    "            unigramCount = trumpTypes[word1] + 1\n",
    "            #if this bigram has been seen before\n",
    "            if word2 in tBigrams[word1].keys():\n",
    "                P = (tBigrams[word1][word2] + 1) / (unigramCount + len(tBigrams[word1]))\n",
    "                #maybe log it? FIGURE OUT LATER\n",
    "            #the second word has not followed the first word before\n",
    "            else:\n",
    "                P = 1 / (unigramCount + len(tBigrams[word1]))\n",
    "        #this unigram has not been seen before\n",
    "        else:\n",
    "            P = unigramCount / (len(trumpTokens)+len(trumpTypes))\n",
    "        return P\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from math import log, exp\n",
    "'''Need to calculate the following:\n",
    "Classifier:\n",
    "P(s|T) = (P(s)*P(T|s))/P(T)\n",
    "T is the text of the tweet and s is the sentiment\n",
    "\n",
    "P(T|s) = P(G|s) = product(for every g in G)(P(g|s))\n",
    "Where G is the set of n-grams and g is an individual n-gram\n",
    "Calculate P(g|s) using +1 for 0 values\n",
    "\n",
    "Calculate P(T) for all T\n",
    "Then add all g to sets s with counts\n",
    "'''\n",
    "\n",
    "#probS is number of tweets in that sentiment divided by total number of tweets\n",
    "\n",
    "def classify(tweet):\n",
    "    #calculate probT\n",
    "    probT = 1/(len(hillTweets) + len(trumpTweets))\n",
    "    #probS is number of tweets in that sentiment divided by total number of tweets\n",
    "    probHill = len(hillTweets)/(len(hillTweets) + len(trumpTweets))\n",
    "    probTrump = 1 - probHill\n",
    "    probHillTweet = (probHill * classifyTweet(tweet, True))/probT\n",
    "    probTrumpTweet = (probTrump * classifyTweet(tweet, False))/probT\n",
    "    return probHillTweet, probTrumpTweet\n",
    "    \n",
    "def classifyTweet(tweet, sentiment):\n",
    "    tweetTokens = tweet.strip().split(' ')\n",
    "    totalProb = numpy.float32(1.0)\n",
    "    for i in range(0, len(tweetTokens) - 1):\n",
    "        totalProb = totalProb * smoothProb(tweetTokens[i], tweetTokens[i+1], sentiment)\n",
    "    return totalProb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.9125904865526194e-12, 0.090088260775395279)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate the bigrams dictionary\n",
    "bothBigrams, hBigrams, tBigrams = generateBigramsDict()\n",
    "\n",
    "classify('<start> make america great again <end>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
