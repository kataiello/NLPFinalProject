{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Need to calculate the following:\n",
    "Classifier:\n",
    "P(s|T) = (P(s)*P(T|s))/P(T)\n",
    "T is the text of the tweet and s is the sentiment\n",
    "\n",
    "P(T|s) = P(G|s) = product(for every g in G)(P(g|s))\n",
    "Where G is the set of n-grams and g is an individual n-gram\n",
    "Calculate P(g|s) using +1 for 0 values\n",
    "\n",
    "Calculate P(T) for all T\n",
    "Then add all g to sets s with counts\n",
    "'''\n",
    "\n",
    "\n",
    "#haven't collected corpus yet, assume this will be filled in\n",
    "#LOAD COUNTS OF PROCESSED TWEETS HERE \n",
    "\n",
    "#Load tokens files for bigrams\n",
    "with open('hill_tokens.txt') as f:\n",
    "    hillTokensFile = f.read()\n",
    "    \n",
    "hillTokens = hillTokensFile.strip().split('\\n')\n",
    "\n",
    "with open('trump_tokens.txt') as f:\n",
    "    trumpTokensFile = f.read()\n",
    "    \n",
    "trumpTokens = trumpTokensFile.strip().split('\\n')\n",
    "\n",
    "#LOAD COUNTS OF TYPES IN EACH HERE AS HILLTYPES, BOTHTYPES, AND TRUMPTYPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9892174006000001, 0.0034346941000000003]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for every word, add it to the dictionary in this format:\n",
    "#key is the word, value is another dictionary\n",
    "    #within each subdictionary, have the following word as the key\n",
    "    #and the number of times it follows as the value\n",
    "\n",
    "#make a bigrams dictionary for both and each candidate\n",
    "bigrams = {}\n",
    "hillBigrams = {}\n",
    "trumpBigrams = {}\n",
    "\n",
    "#def generateBigramsDict():\n",
    "#Add all Hillary positive bigrams to both hillBigrams and bigrams\n",
    "for i in range(len(hillTokens) - 1):\n",
    "    #if the word already exists\n",
    "    if hillTokens[i] in hillBigrams:\n",
    "        #if the following word already exists\n",
    "        if hillTokens[i+1] in hillBigrams[hillTokens[i]]:\n",
    "            #add 1 to the number of occurences\n",
    "            hillBigrams[hillTokens[i]][hillTokens[i+1]] += 1\n",
    "            bigrams[hillTokens[i]][hillTokens[i+1]] += 1\n",
    "        #add it to the subdictionary of hill\n",
    "        else:\n",
    "            hillBigrams[hillTokens[i]][hillTokens[i+1]] = 1\n",
    "            bigrams[hillTokens[i]][hillTokens[i+1]] = 1\n",
    "    #the word does not exist yet in hillBigrams\n",
    "    else:\n",
    "        #initialize a new subdictionary and add the first bigram\n",
    "        hillBigrams[hillTokens[i]] = {hillTokens[i+1]: 1}\n",
    "        bigrams[hillTokens[i]] = {hillTokens[i+1]: 1}\n",
    "\n",
    "#Add all Trump positive bigrams to both trumpBigrams and bigrams\n",
    "for i in range(len(trumpTokens) - 1):\n",
    "    #if the word already exists\n",
    "    if trumpTokens[i] in trumpBigrams:\n",
    "        #if the following word already exists\n",
    "        if trumpTokens[i+1] in trumpBigrams[trumpTokens[i]]:\n",
    "            #add 1 to the number of occurences\n",
    "            trumpBigrams[trumpTokens[i]][trumpTokens[i+1]] += 1\n",
    "            bigrams[trumpTokens[i]][trumpTokens[i+1]] += 1\n",
    "        #add it to the subdictionary of trump\n",
    "        else:\n",
    "            trumpBigrams[trumpTokens[i]][trumpTokens[i+1]] = 1\n",
    "            #check bigrams\n",
    "            if trumpTokens[i+1] in bigrams[trumpTokens[i]]:\n",
    "                bigrams[trumpTokens[i]][trumpTokens[i+1]] += 1\n",
    "            else:\n",
    "                bigrams[trumpTokens[i]][trumpTokens[i+1]] = 1\n",
    "    #the word does not exist yet in trumpBigrams\n",
    "    else:\n",
    "        #initialize a new subdictionary and add the first bigram\n",
    "        trumpBigrams[trumpTokens[i]] = {trumpTokens[i+1]: 1}\n",
    "        #check bigrams\n",
    "        if trumpTokens[i] in bigrams:\n",
    "            #check word\n",
    "            if trumpTokens[i+1] in bigrams[trumpTokens[i]]:\n",
    "                bigrams[trumpTokens[i]][trumpTokens[i+1]] += 1\n",
    "            else:\n",
    "                bigrams[trumpTokens[i]][trumpTokens[i+1]] = 1\n",
    "        else:\n",
    "            bigrams[trumpTokens[i]] = {trumpTokens[i+1]: 1}\n",
    "            \n",
    "'''Need to implement Good-Turing smoothing\n",
    "Yikes I really don't want to do that\n",
    "Try Laplace first?'''\n",
    "\n",
    "\n",
    "'''MLE = count of bigram / count of first unigram\n",
    "Laplace: P = (count of bigram + 1)/(count of first unigram + V)\n",
    "where V is the number of possible bigrams given unigram\n",
    "What do we do if the given unigram doesn't exist?'''\n",
    "def smoothProb(word1, word2, sentiment):\n",
    "    #sentiment 0 = bigrams\n",
    "    if sentiment == 0:\n",
    "        \n",
    "    #sentiment 1 = hillary\n",
    "    elif sentiment == 1:\n",
    "        #add one for smoothing\n",
    "        unigramCount = hillTypes[word1] + 1\n",
    "        #if the unigram has been seen before\n",
    "        if word1 in hillBigrams.keys():\n",
    "            #if this bigram has been seen before\n",
    "            if word2 in hillBigrams[word1].keys():\n",
    "                P = (hillBigrams[word1][word2] + 1) / (unigramCount + len(hillBigrams[word1]))\n",
    "                #maybe log it?\n",
    "            #the second word has not followed the first word before\n",
    "            else:\n",
    "                P = 1 / (unigramCount + len(hillBigrams[word1]))\n",
    "        #this unigram has not been seen before\n",
    "        else:\n",
    "            P = \n",
    "        \n",
    "    \n",
    "    \n",
    "'''\n",
    "P(g|s) = P(g and s) / P(s)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Need to calculate the following:\n",
    "Classifier:\n",
    "P(s|T) = (P(s)*P(T|s))/P(T)\n",
    "T is the text of the tweet and s is the sentiment\n",
    "\n",
    "P(T|s) = P(G|s) = product(for every g in G)(P(g|s))\n",
    "Where G is the set of n-grams and g is an individual n-gram\n",
    "Calculate P(g|s) using +1 for 0 values\n",
    "\n",
    "Calculate P(T) for all T\n",
    "Then add all g to sets s with counts\n",
    "'''\n",
    "\n",
    "#probS is number of tweets in that sentiment divided by total number of tweets\n",
    "\n",
    "def classify(tweet):\n",
    "    #calculate probT\n",
    "    probT = tweetCount(tweet)/len(tweetCount)\n",
    "    #probS is number of tweets in that sentiment divided by total number of tweets\n",
    "    probS = 0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
