{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.chdir('/Users/kataiello/Documents/School_Work/University/MoHo/Third_Year/Spring/cs341-nlp/NLPFinalProject/')\n",
    "projectPath = os.getcwd()\n",
    "os.chdir('/Users/kataiello/Documents/School_Work/University/MoHo/Third_Year/Spring/cs341-nlp/NLPFinalProject/textFiles/training/')\n",
    "trainingPath = os.getcwd()\n",
    "os.chdir('/Users/kataiello/Documents/School_Work/University/MoHo/Third_Year/Spring/cs341-nlp/NLPFinalProject/textFiles/testing/')\n",
    "testingPath = os.getcwd()\n",
    "os.chdir('/Users/kataiello/Documents/School_Work/University/MoHo/Third_Year/Spring/cs341-nlp/NLPFinalProject/textFiles/output/')\n",
    "outputPath = os.getcwd()\n",
    "#print(projectPath)\n",
    "#print(trainingPath)\n",
    "#print(testingPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "'''Need to calculate the following:\n",
    "Classifier:\n",
    "P(s|T) = (P(s)*P(T|s))/P(T)\n",
    "T is the text of the tweet and s is the sentiment\n",
    "\n",
    "P(T|s) = P(G|s) = product(for every g in G)(P(g|s))\n",
    "Where G is the set of n-grams and g is an individual n-gram\n",
    "Calculate P(g|s) using +1 for 0 values\n",
    "\n",
    "Calculate P(T) for all T\n",
    "Then add all g to sets s with counts\n",
    "'''\n",
    "\n",
    "with open(trainingPath+'/hillarytraining.txt') as f:\n",
    "    hillRawTweets = f.read()\n",
    "hillLines = hillRawTweets.split('\\n')\n",
    "hillTweets = []\n",
    "hillTokens = []\n",
    "hillTypes = {}\n",
    "    \n",
    "for line in hillLines:\n",
    "    #remove @usernames, #hashtags, RTs, links\n",
    "    result = re.sub(r\"(?:@\\S*|#\\S*|RT|http\\S*|pic.\\S*)\", \"\", line)\n",
    "    #set all letters to lowercase\n",
    "    result = result.lower()\n",
    "    #substitute all punctuation with a space\n",
    "    result = re.sub(r\"[^\\w\\s]\",' ',result)\n",
    "    result = re.sub(' +', ' ', result)\n",
    "    result = result.strip()\n",
    "    if result != '':\n",
    "        result = \"<start> \" + result + \" <end>\"\n",
    "        lineTokens = result.split(' ')\n",
    "        for token in lineTokens:\n",
    "            hillTokens.append(token)\n",
    "            if token in hillTypes:\n",
    "                hillTypes[token] = hillTypes[token] + 1\n",
    "            else:\n",
    "                hillTypes[token] = 1\n",
    "        hillTweets.append(result)\n",
    "\n",
    "        \n",
    "with open(trainingPath+'/trumptraining.txt') as f:\n",
    "    trumpRawTweets = f.read()\n",
    "trumpLines = trumpRawTweets.split('\\n')\n",
    "trumpTweets = []\n",
    "trumpTokens = []\n",
    "trumpTypes = {}\n",
    "    \n",
    "for line in trumpLines:\n",
    "    #remove @usernames, #hashtags, RTs, links\n",
    "    result = re.sub(r\"(?:@\\S*|#\\S*|RT|http\\S*|pic.\\S*)\", \"\", line)\n",
    "    #set all letters to lowercase\n",
    "    result = result.lower()\n",
    "    #substitute all punctuation with a space\n",
    "    result = re.sub(r\"[^\\w\\s]\",' ',result)\n",
    "    result = re.sub(' +', ' ', result)\n",
    "    result = result.strip()\n",
    "    if result != '':\n",
    "        result = \"<start> \" + result + \" <end>\"\n",
    "        lineTokens = result.split(' ')\n",
    "        for token in lineTokens:\n",
    "            trumpTokens.append(token)\n",
    "            if token in trumpTypes:\n",
    "                trumpTypes[token] = trumpTypes[token] + 1\n",
    "            else:\n",
    "                trumpTypes[token] = 1\n",
    "        trumpTweets.append(result)\n",
    "#print(len(trumpTweets) + len(hillTweets))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import time\n",
    "\n",
    "def runAnalysis(testFile):\n",
    "    with open(testingPath+'/'+testFile+'.txt') as f:\n",
    "        testText = f.read()\n",
    "    testLines = testText.split('\\n')\n",
    "    testTweets = []\n",
    "    \n",
    "    for line in testLines:\n",
    "        #remove @usernames, #hashtags, RTs, links\n",
    "        result = re.sub(r\"(?:@\\S*|#\\S*|RT|http\\S*|pic.\\S*)\", \"\", line)\n",
    "        #set all letters to lowercase\n",
    "        result = result.lower()\n",
    "        #substitute all punctuation with a space\n",
    "        result = re.sub(r\"[^\\w\\s]\",' ',result)\n",
    "        result = re.sub(' +', ' ', result)\n",
    "        result = result.strip()\n",
    "        if result != '':\n",
    "            result = \"<start> \" + result + \" <end>\"\n",
    "            testTweets.append(result)\n",
    "        \n",
    "    outputFile = open(outputPath+'/'+testFile+'Output.txt','w')\n",
    "    outputFile.write('Test using '+testFile+' on '+time.strftime(\"%d/%m/%Y\")+' at '+time.strftime(\"%H:%M:%S\")+'\\n')\n",
    "    totalHill = numpy.float32(0.0)\n",
    "    totalTrump = numpy.float32(0.0)\n",
    "    for tweet in testTweets:\n",
    "        pHill, pTrump = classify(tweet)\n",
    "        #retpHill = pHill * 10**15\n",
    "        #retpTrump = pTrump * 10**15\n",
    "        #print(\"{}: {}, {}\".format(tweet, retpHill, retpTrump))\n",
    "        totalHill += pHill\n",
    "        totalTrump += pTrump\n",
    "        outputFile.write(\"For tweet: {} \\nProbability of supporting \\n Hillary Clinton: {} \\n Donald Trump: {}\".format(tweet, pHill, pTrump)+'\\n')\n",
    "    proHill = (totalHill / (totalHill + totalTrump)) * 100\n",
    "    proTrump = (totalTrump / (totalHill + totalTrump)) * 100\n",
    "    print(\"% supporting Hillary Clinton: {} \\n% supporting Donald Trump: {}\".format(proHill, proTrump))\n",
    "    outputFile.write('\\n\\n'+\"% supporting Hillary Clinton: {} \\n% supporting Donald Trump: {}\".format(proHill, proTrump))\n",
    "    outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#Load tokens files for bigrams\\nwith open('hill_tokens.txt') as f:\\n    hillTokensFile = f.read()\\n    \\nhillTokens = hillTokensFile.strip().split('\\n')\\n\\nwith open('trump_tokens.txt') as f:\\n    trumpTokensFile = f.read()\\n    \\ntrumpTokens = trumpTokensFile.strip().split('\\n')\\n\\n\\n#Load counts of types here as hillTypes and trumpTypes\\nwith open('hill_counts.txt') as f:\\n    hCountsFile = f.read()\\nhCounts = hCountsFile.strip().split('\\n')\\nhillTypes = {}\\nfor i in range (len(hCounts)):\\n    hCounts[i] = hCounts[i].lstrip()\\n    hillTypes[hCounts[i].split(' ')[1]] = int(hCounts[i].split(' ')[0]) \\n    \\nwith open('trump_counts.txt') as f:\\n    tCountsFile = f.read()\\ntCounts = tCountsFile.strip().split('\\n')\\ntrumpTypes = {}\\nfor i in range (len(tCounts)):\\n    tCounts[i] = tCounts[i].lstrip()\\n    trumpTypes[tCounts[i].split(' ')[1]] = int(tCounts[i].split(' ')[0]) \\n    \\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#haven't collected corpus yet, assume this will be filled in\n",
    "#Load processed tweets here\n",
    "with open('hillProcessed.txt') as f:\n",
    "    hillProcessed = f.read()\n",
    "hillTweets = hillProcessed.split('\\n')'''\n",
    "\n",
    "\n",
    "'''with open('trumpProcessed.txt') as f:\n",
    "    trumpProcessed = f.read()\n",
    "trumpTweets = trumpProcessed.split('\\n')'''\n",
    "\n",
    "\n",
    "'''#Load tokens files for bigrams\n",
    "with open('hill_tokens.txt') as f:\n",
    "    hillTokensFile = f.read()\n",
    "    \n",
    "hillTokens = hillTokensFile.strip().split('\\n')\n",
    "\n",
    "with open('trump_tokens.txt') as f:\n",
    "    trumpTokensFile = f.read()\n",
    "    \n",
    "trumpTokens = trumpTokensFile.strip().split('\\n')\n",
    "\n",
    "\n",
    "#Load counts of types here as hillTypes and trumpTypes\n",
    "with open('hill_counts.txt') as f:\n",
    "    hCountsFile = f.read()\n",
    "hCounts = hCountsFile.strip().split('\\n')\n",
    "hillTypes = {}\n",
    "for i in range (len(hCounts)):\n",
    "    hCounts[i] = hCounts[i].lstrip()\n",
    "    hillTypes[hCounts[i].split(' ')[1]] = int(hCounts[i].split(' ')[0]) \n",
    "    \n",
    "with open('trump_counts.txt') as f:\n",
    "    tCountsFile = f.read()\n",
    "tCounts = tCountsFile.strip().split('\\n')\n",
    "trumpTypes = {}\n",
    "for i in range (len(tCounts)):\n",
    "    tCounts[i] = tCounts[i].lstrip()\n",
    "    trumpTypes[tCounts[i].split(' ')[1]] = int(tCounts[i].split(' ')[0]) \n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for every word, add it to the dictionary in this format:\n",
    "#key is the word, value is another dictionary\n",
    "    #within each subdictionary, have the following word as the key\n",
    "    #and the number of times it follows as the value\n",
    "\n",
    "def generateBigramsDict():\n",
    "    #make a bigrams dictionary for both and each candidate\n",
    "    bigrams = {}\n",
    "    hillBigrams = {}\n",
    "    trumpBigrams = {}\n",
    "    #Add all Hillary positive bigrams to both hillBigrams and bigrams\n",
    "    for i in range(len(hillTokens) - 1):\n",
    "        #if the word already exists\n",
    "        if hillTokens[i] in hillBigrams:\n",
    "            #if the following word already exists\n",
    "            if hillTokens[i+1] in hillBigrams[hillTokens[i]]:\n",
    "                #add 1 to the number of occurences\n",
    "                hillBigrams[hillTokens[i]][hillTokens[i+1]] += 1\n",
    "                bigrams[hillTokens[i]][hillTokens[i+1]] += 1\n",
    "            #add it to the subdictionary of hill\n",
    "            else:\n",
    "                hillBigrams[hillTokens[i]][hillTokens[i+1]] = 1\n",
    "                bigrams[hillTokens[i]][hillTokens[i+1]] = 1\n",
    "        #the word does not exist yet in hillBigrams\n",
    "        else:\n",
    "            #initialize a new subdictionary and add the first bigram\n",
    "            hillBigrams[hillTokens[i]] = {hillTokens[i+1]: 1}\n",
    "            bigrams[hillTokens[i]] = {hillTokens[i+1]: 1}\n",
    "\n",
    "    #Add all Trump positive bigrams to both trumpBigrams and bigrams\n",
    "    for i in range(len(trumpTokens) - 1):\n",
    "        #if the word already exists\n",
    "        if trumpTokens[i] in trumpBigrams:\n",
    "            #if the following word already exists\n",
    "            if trumpTokens[i+1] in trumpBigrams[trumpTokens[i]]:\n",
    "                #add 1 to the number of occurences\n",
    "                trumpBigrams[trumpTokens[i]][trumpTokens[i+1]] += 1\n",
    "                bigrams[trumpTokens[i]][trumpTokens[i+1]] += 1\n",
    "            #add it to the subdictionary of trump\n",
    "            else:\n",
    "                trumpBigrams[trumpTokens[i]][trumpTokens[i+1]] = 1\n",
    "                #check bigrams\n",
    "                if trumpTokens[i+1] in bigrams[trumpTokens[i]]:\n",
    "                    bigrams[trumpTokens[i]][trumpTokens[i+1]] += 1\n",
    "                else:\n",
    "                    bigrams[trumpTokens[i]][trumpTokens[i+1]] = 1\n",
    "        #the word does not exist yet in trumpBigrams\n",
    "        else:\n",
    "            #initialize a new subdictionary and add the first bigram\n",
    "            trumpBigrams[trumpTokens[i]] = {trumpTokens[i+1]: 1}\n",
    "            #check bigrams\n",
    "            if trumpTokens[i] in bigrams:\n",
    "                #check word\n",
    "                if trumpTokens[i+1] in bigrams[trumpTokens[i]]:\n",
    "                    bigrams[trumpTokens[i]][trumpTokens[i+1]] += 1\n",
    "                else:\n",
    "                    bigrams[trumpTokens[i]][trumpTokens[i+1]] = 1\n",
    "            else:\n",
    "                bigrams[trumpTokens[i]] = {trumpTokens[i+1]: 1}\n",
    "    return bigrams, hillBigrams, trumpBigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import float32\n",
    "from math import log, exp\n",
    "\n",
    "'''Need to implement Laplace smoothing'''\n",
    "\n",
    "\n",
    "'''MLE = count of bigram / count of first unigram\n",
    "Laplace: P = (count of bigram + 1)/(count of first unigram + V)\n",
    "where V is the number of possible bigrams given unigram\n",
    "What do we do if the given unigram doesn't exist?'''\n",
    "\n",
    "def smoothProb(word1, word2, sentiment):\n",
    "    P = numpy.float32(0.0)\n",
    "  \n",
    "    #True = hillary\n",
    "    if sentiment == True:\n",
    "        unigramCount = 1\n",
    "        #if the unigram has been seen before\n",
    "        if word1 in hBigrams.keys():\n",
    "            #add one for smoothing\n",
    "            unigramCount = hillTypes[word1] + 1\n",
    "            #if this bigram has been seen before\n",
    "            if word2 in hBigrams[word1].keys():\n",
    "                P = (hBigrams[word1][word2] + 1) / (unigramCount + len(hBigrams[word1]))\n",
    "                #maybe log it? FIGURE OUT LATER\n",
    "            #the second word has not followed the first word before\n",
    "            else:\n",
    "                P = 1 / (unigramCount + len(hBigrams[word1]))\n",
    "        #this unigram has not been seen before\n",
    "        else:\n",
    "            P = unigramCount / (len(hillTokens)+len(hillTypes))\n",
    "        return P\n",
    "    #False = trump\n",
    "    else:\n",
    "        unigramCount = 1\n",
    "        #if the unigram has been seen before\n",
    "        if word1 in tBigrams.keys():\n",
    "            #add one for smoothing\n",
    "            unigramCount = trumpTypes[word1] + 1\n",
    "            #if this bigram has been seen before\n",
    "            if word2 in tBigrams[word1].keys():\n",
    "                P = (tBigrams[word1][word2] + 1) / (unigramCount + len(tBigrams[word1]))\n",
    "                #maybe log it? FIGURE OUT LATER\n",
    "            #the second word has not followed the first word before\n",
    "            else:\n",
    "                P = 1 / (unigramCount + len(tBigrams[word1]))\n",
    "        #this unigram has not been seen before\n",
    "        else:\n",
    "            P = unigramCount / (len(trumpTokens)+len(trumpTypes))\n",
    "        return P\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from math import log, exp\n",
    "'''Need to calculate the following:\n",
    "Classifier:\n",
    "P(s|T) = (P(s)*P(T|s))/P(T)\n",
    "T is the text of the tweet and s is the sentiment\n",
    "\n",
    "P(T|s) = P(G|s) = product(for every g in G)(P(g|s))\n",
    "Where G is the set of n-grams and g is an individual n-gram\n",
    "Calculate P(g|s) using +1 for 0 values\n",
    "\n",
    "Calculate P(T) for all T\n",
    "Then add all g to sets s with counts\n",
    "'''\n",
    "\n",
    "#probS is number of tweets in that sentiment divided by total number of tweets\n",
    "\n",
    "def classify(tweet):\n",
    "    #calculate probT\n",
    "    #probT = 1/(len(hillTweets) + len(trumpTweets))\n",
    "    #probS is number of tweets in that sentiment divided by total number of tweets\n",
    "    probHill = len(hillTweets)/(len(hillTweets) + len(trumpTweets))\n",
    "    probTrump = 1 - probHill\n",
    "    probHillTweet = (probHill * classifyTweet(tweet, True))\n",
    "    probTrumpTweet = (probTrump * classifyTweet(tweet, False))\n",
    "    #return (probHillTweet / (probHillTweet + probTrumpTweet)), (probTrumpTweet / (probHillTweet + probTrumpTweet))\n",
    "    return probHillTweet, probTrumpTweet\n",
    "    \n",
    "def classifyTweet(tweet, sentiment):\n",
    "    tweetTokens = tweet.strip().split(' ')\n",
    "    totalProb = numpy.float32(1.0)\n",
    "    for i in range(0, len(tweetTokens) - 1):\n",
    "        totalProb = totalProb * smoothProb(tweetTokens[i], tweetTokens[i+1], sentiment)\n",
    "    return totalProb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% supporting Hillary Clinton: 49.601978366195794 \n",
      "% supporting Donald Trump: 50.398021633804206\n",
      "% supporting Hillary Clinton: 25.504968543220098 \n",
      "% supporting Donald Trump: 74.4950314567799\n",
      "% supporting Hillary Clinton: 37.52399285964109 \n",
      "% supporting Donald Trump: 62.476007140358924\n",
      "% supporting Hillary Clinton: 86.74745232579279 \n",
      "% supporting Donald Trump: 13.25254767420721\n",
      "% supporting Hillary Clinton: 15.391528073842068 \n",
      "% supporting Donald Trump: 84.60847192615793\n",
      "% supporting Hillary Clinton: 53.581041884171874 \n",
      "% supporting Donald Trump: 46.418958115828126\n",
      "% supporting Hillary Clinton: 73.58531022307618 \n",
      "% supporting Donald Trump: 26.41468977692383\n"
     ]
    }
   ],
   "source": [
    "#generate the bigrams dictionary\n",
    "bothBigrams, hBigrams, tBigrams = generateBigramsDict()\n",
    "\n",
    "classify('<start> make america great again <end>')\n",
    "#runAnalysis('hillaryTweetsTest.txt')\n",
    "#runAnalysis('tweetsTest.txt')\n",
    "runAnalysis('debate926')\n",
    "runAnalysis('debate109')\n",
    "runAnalysis('debate1019')\n",
    "runAnalysis('namesdebate926')\n",
    "runAnalysis('namesdebate109')\n",
    "runAnalysis('namesdebate1019')\n",
    "runAnalysis('debate926nonames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> any bets on how long trump marriage number 3 will last after the campaign <end>\n",
      "<start> she s ill <end>\n"
     ]
    }
   ],
   "source": [
    "print(hillTweets[0])\n",
    "print(trumpTweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
